{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'inception'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e1946b71fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0minception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscopes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'inception'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from inception.slim import scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scopes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ff4665e9637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[1;33m@\u001b[0m\u001b[0mscopes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_arg_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \"\"\"Returns the global step variable.\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scopes' is not defined"
     ]
    }
   ],
   "source": [
    "# Collection containing all the variables created using slim.variables\n",
    "MODEL_VARIABLES = '_model_variables_'\n",
    "\n",
    "# Collection containing the slim.variables that are created with restore=True.\n",
    "VARIABLES_TO_RESTORE = '_variables_to_restore_'\n",
    "\n",
    "\n",
    "def add_variable(var, restore=True):\n",
    "  \"\"\"Adds a variable to the MODEL_VARIABLES collection.\n",
    "    Optionally it will add the variable to  the VARIABLES_TO_RESTORE collection.\n",
    "  Args:\n",
    "    var: a variable.\n",
    "    restore: whether the variable should be added to the\n",
    "      VARIABLES_TO_RESTORE collection.\n",
    "  \"\"\"\n",
    "  collections = [MODEL_VARIABLES]\n",
    "  if restore:\n",
    "    collections.append(VARIABLES_TO_RESTORE)\n",
    "  for collection in collections:\n",
    "    if var not in tf.get_collection(collection):\n",
    "      tf.add_to_collection(collection, var)\n",
    "\n",
    "\n",
    "def get_variables(scope=None, suffix=None):\n",
    "  \"\"\"Gets the list of variables, filtered by scope and/or suffix.\n",
    "  Args:\n",
    "    scope: an optional scope for filtering the variables to return.\n",
    "    suffix: an optional suffix for filtering the variables to return.\n",
    "  Returns:\n",
    "    a copied list of variables with scope and suffix.\n",
    "  \"\"\"\n",
    "  candidates = tf.get_collection(MODEL_VARIABLES, scope)[:]\n",
    "  if suffix is not None:\n",
    "    candidates = [var for var in candidates if var.op.name.endswith(suffix)]\n",
    "  return candidates\n",
    "\n",
    "\n",
    "def get_variables_to_restore():\n",
    "  \"\"\"Gets the list of variables to restore.\n",
    "  Returns:\n",
    "    a copied list of variables.\n",
    "  \"\"\"\n",
    "  return tf.get_collection(VARIABLES_TO_RESTORE)[:]\n",
    "\n",
    "\n",
    "def get_variables_by_name(given_name, scope=None):\n",
    "  \"\"\"Gets the list of variables that were given that name.\n",
    "  Args:\n",
    "    given_name: name given to the variable without scope.\n",
    "    scope: an optional scope for filtering the variables to return.\n",
    "  Returns:\n",
    "    a copied list of variables with the given name and prefix.\n",
    "  \"\"\"\n",
    "  return get_variables(scope=scope, suffix=given_name)\n",
    "\n",
    "\n",
    "def get_unique_variable(name):\n",
    "  \"\"\"Gets the variable uniquely identified by that name.\n",
    "  Args:\n",
    "    name: a name that uniquely identifies the variable.\n",
    "  Returns:\n",
    "    a tensorflow variable.\n",
    "  Raises:\n",
    "    ValueError: if no variable uniquely identified by the name exists.\n",
    "  \"\"\"\n",
    "  candidates = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, name)\n",
    "  if not candidates:\n",
    "    raise ValueError('Couldnt find variable %s' % name)\n",
    "\n",
    "  for candidate in candidates:\n",
    "    if candidate.op.name == name:\n",
    "      return candidate\n",
    "  raise ValueError('Variable %s does not uniquely identify a variable', name)\n",
    "\n",
    "\n",
    "class VariableDeviceChooser(object):\n",
    "  \"\"\"Slim device chooser for variables.\n",
    "  When using a parameter server it will assign them in a round-robin fashion.\n",
    "  When not using a parameter server it allows GPU:0 placement otherwise CPU:0.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_parameter_servers=0,\n",
    "               ps_device='/job:ps',\n",
    "               placement='CPU:0'):\n",
    "    \"\"\"Initialize VariableDeviceChooser.\n",
    "    Args:\n",
    "      num_parameter_servers: number of parameter servers.\n",
    "      ps_device: string representing the parameter server device.\n",
    "      placement: string representing the placement of the variable either CPU:0\n",
    "        or GPU:0. When using parameter servers forced to CPU:0.\n",
    "    \"\"\"\n",
    "    self._num_ps = num_parameter_servers\n",
    "    self._ps_device = ps_device\n",
    "    self._placement = placement if num_parameter_servers == 0 else 'CPU:0'\n",
    "    self._next_task_id = 0\n",
    "\n",
    "  def __call__(self, op):\n",
    "    device_string = ''\n",
    "    if self._num_ps > 0:\n",
    "      task_id = self._next_task_id\n",
    "      self._next_task_id = (self._next_task_id + 1) % self._num_ps\n",
    "      device_string = '%s/task:%d' % (self._ps_device, task_id)\n",
    "    device_string += '/%s' % self._placement\n",
    "    return device_string\n",
    "\n",
    "\n",
    "# TODO(sguada) Remove once get_variable is able to colocate op.devices.\n",
    "def variable_device(device, name):\n",
    "  \"\"\"Fix the variable device to colocate its ops.\"\"\"\n",
    "  if callable(device):\n",
    "    var_name = tf.get_variable_scope().name + '/' + name\n",
    "    var_def = tf.NodeDef(name=var_name, op='Variable')\n",
    "    device = device(var_def)\n",
    "  if device is None:\n",
    "    device = ''\n",
    "  return device\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def global_step(device=''):\n",
    "  \"\"\"Returns the global step variable.\n",
    "  Args:\n",
    "    device: Optional device to place the variable. It can be an string or a\n",
    "      function that is called to get the device for the variable.\n",
    "  Returns:\n",
    "    the tensor representing the global step variable.\n",
    "  \"\"\"\n",
    "  global_step_ref = tf.get_collection(tf.GraphKeys.GLOBAL_STEP)\n",
    "  if global_step_ref:\n",
    "    return global_step_ref[0]\n",
    "  else:\n",
    "    collections = [\n",
    "        VARIABLES_TO_RESTORE,\n",
    "        tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "        tf.GraphKeys.GLOBAL_STEP,\n",
    "    ]\n",
    "    # Get the device for the variable.\n",
    "    with tf.device(variable_device(device, 'global_step')):\n",
    "      return tf.get_variable('global_step', shape=[], dtype=tf.int64,\n",
    "                             initializer=tf.zeros_initializer(),\n",
    "                             trainable=False, collections=collections)\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def variable(name, shape=None, dtype=tf.float32, initializer=None,\n",
    "             regularizer=None, trainable=True, collections=None, device='',\n",
    "             restore=True):\n",
    "  \"\"\"Gets an existing variable with these parameters or creates a new one.\n",
    "    It also add itself to a group with its name.\n",
    "  Args:\n",
    "    name: the name of the new or existing variable.\n",
    "    shape: shape of the new or existing variable.\n",
    "    dtype: type of the new or existing variable (defaults to `DT_FLOAT`).\n",
    "    initializer: initializer for the variable if one is created.\n",
    "    regularizer: a (Tensor -> Tensor or None) function; the result of\n",
    "        applying it on a newly created variable will be added to the collection\n",
    "        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n",
    "    trainable: If `True` also add the variable to the graph collection\n",
    "      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
    "    collections: A list of collection names to which the Variable will be added.\n",
    "      Note that the variable is always also added to the tf.GraphKeys.GLOBAL_VARIABLES\n",
    "      and MODEL_VARIABLES collections.\n",
    "    device: Optional device to place the variable. It can be an string or a\n",
    "      function that is called to get the device for the variable.\n",
    "    restore: whether the variable should be added to the\n",
    "      VARIABLES_TO_RESTORE collection.\n",
    "  Returns:\n",
    "    The created or existing variable.\n",
    "  \"\"\"\n",
    "  collections = list(collections or [])\n",
    "\n",
    "  # Make sure variables are added to tf.GraphKeys.GLOBAL_VARIABLES and MODEL_VARIABLES\n",
    "  collections += [tf.GraphKeys.GLOBAL_VARIABLES, MODEL_VARIABLES]\n",
    "  # Add to VARIABLES_TO_RESTORE if necessary\n",
    "  if restore:\n",
    "    collections.append(VARIABLES_TO_RESTORE)\n",
    "  # Remove duplicates\n",
    "  collections = set(collections)\n",
    "  # Get the device for the variable.\n",
    "  with tf.device(variable_device(device, name)):\n",
    "    return tf.get_variable(name, shape=shape, dtype=dtype,\n",
    "                           initializer=initializer, regularizer=regularizer,\n",
    "                           trainable=trainable, collections=collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
