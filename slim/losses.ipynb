{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to gather all losses in a network, the user should use this\n",
    "# key for get_collection, i.e:\n",
    "#   losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n",
    "LOSSES_COLLECTION = '_losses'\n",
    "\n",
    "\n",
    "def l1_regularizer(weight=1.0, scope=None):\n",
    "  \"\"\"Define a L1 regularizer.\n",
    "  Args:\n",
    "    weight: scale the loss by this factor.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a regularizer function.\n",
    "  \"\"\"\n",
    "  def regularizer(tensor):\n",
    "    with tf.name_scope(scope, 'L1Regularizer', [tensor]):\n",
    "      l1_weight = tf.convert_to_tensor(weight,\n",
    "                                       dtype=tensor.dtype.base_dtype,\n",
    "                                       name='weight')\n",
    "      return tf.multiply(l1_weight, tf.reduce_sum(tf.abs(tensor)), name='value')\n",
    "  return regularizer\n",
    "\n",
    "\n",
    "def l2_regularizer(weight=1.0, scope=None):\n",
    "  \"\"\"Define a L2 regularizer.\n",
    "  Args:\n",
    "    weight: scale the loss by this factor.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a regularizer function.\n",
    "  \"\"\"\n",
    "  def regularizer(tensor):\n",
    "    with tf.name_scope(scope, 'L2Regularizer', [tensor]):\n",
    "      l2_weight = tf.convert_to_tensor(weight,\n",
    "                                       dtype=tensor.dtype.base_dtype,\n",
    "                                       name='weight')\n",
    "      return tf.multiply(l2_weight, tf.nn.l2_loss(tensor), name='value')\n",
    "  return regularizer\n",
    "\n",
    "\n",
    "def l1_l2_regularizer(weight_l1=1.0, weight_l2=1.0, scope=None):\n",
    "  \"\"\"Define a L1L2 regularizer.\n",
    "  Args:\n",
    "    weight_l1: scale the L1 loss by this factor.\n",
    "    weight_l2: scale the L2 loss by this factor.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a regularizer function.\n",
    "  \"\"\"\n",
    "  def regularizer(tensor):\n",
    "    with tf.name_scope(scope, 'L1L2Regularizer', [tensor]):\n",
    "      weight_l1_t = tf.convert_to_tensor(weight_l1,\n",
    "                                         dtype=tensor.dtype.base_dtype,\n",
    "                                         name='weight_l1')\n",
    "      weight_l2_t = tf.convert_to_tensor(weight_l2,\n",
    "                                         dtype=tensor.dtype.base_dtype,\n",
    "                                         name='weight_l2')\n",
    "      reg_l1 = tf.multiply(weight_l1_t, tf.reduce_sum(tf.abs(tensor)),\n",
    "                      name='value_l1')\n",
    "      reg_l2 = tf.multiply(weight_l2_t, tf.nn.l2_loss(tensor),\n",
    "                      name='value_l2')\n",
    "      return tf.add(reg_l1, reg_l2, name='value')\n",
    "  return regularizer\n",
    "\n",
    "\n",
    "def l1_loss(tensor, weight=1.0, scope=None):\n",
    "  \"\"\"Define a L1Loss, useful for regularize, i.e. lasso.\n",
    "  Args:\n",
    "    tensor: tensor to regularize.\n",
    "    weight: scale the loss by this factor.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    the L1 loss op.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'L1Loss', [tensor]):\n",
    "    weight = tf.convert_to_tensor(weight,\n",
    "                                  dtype=tensor.dtype.base_dtype,\n",
    "                                  name='loss_weight')\n",
    "    loss = tf.multiply(weight, tf.reduce_sum(tf.abs(tensor)), name='value')\n",
    "    tf.add_to_collection(LOSSES_COLLECTION, loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def l2_loss(tensor, weight=1.0, scope=None):\n",
    "  \"\"\"Define a L2Loss, useful for regularize, i.e. weight decay.\n",
    "  Args:\n",
    "    tensor: tensor to regularize.\n",
    "    weight: an optional weight to modulate the loss.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    the L2 loss op.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'L2Loss', [tensor]):\n",
    "    weight = tf.convert_to_tensor(weight,\n",
    "                                  dtype=tensor.dtype.base_dtype,\n",
    "                                  name='loss_weight')\n",
    "    loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name='value')\n",
    "    tf.add_to_collection(LOSSES_COLLECTION, loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, one_hot_labels, label_smoothing=0,\n",
    "                       weight=1.0, scope=None):\n",
    "  \"\"\"Define a Cross Entropy loss using softmax_cross_entropy_with_logits.\n",
    "  It can scale the loss by weight factor, and smooth the labels.\n",
    "  Args:\n",
    "    logits: [batch_size, num_classes] logits outputs of the network .\n",
    "    one_hot_labels: [batch_size, num_classes] target one_hot_encoded labels.\n",
    "    label_smoothing: if greater than 0 then smooth the labels.\n",
    "    weight: scale the loss by this factor.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    A tensor with the softmax_cross_entropy loss.\n",
    "  \"\"\"\n",
    "  logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n",
    "  with tf.name_scope(scope, 'CrossEntropyLoss', [logits, one_hot_labels]):\n",
    "    num_classes = one_hot_labels.get_shape()[-1].value\n",
    "    one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n",
    "    if label_smoothing > 0:\n",
    "      smooth_positives = 1.0 - label_smoothing\n",
    "      smooth_negatives = label_smoothing / num_classes\n",
    "      one_hot_labels = one_hot_labels * smooth_positives + smooth_negatives\n",
    "    cross_entropy = tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits(\n",
    "        logits, one_hot_labels, name='xentropy')\n",
    "\n",
    "    weight = tf.convert_to_tensor(weight,\n",
    "                                  dtype=logits.dtype.base_dtype,\n",
    "                                  name='loss_weight')\n",
    "    loss = tf.multiply(weight, tf.reduce_mean(cross_entropy), name='value')\n",
    "    tf.add_to_collection(LOSSES_COLLECTION, loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
