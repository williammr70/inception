{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'inception'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-766d2cf8e949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0minception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscopes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'inception'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.training import moving_averages\n",
    "\n",
    "from inception.slim import losses\n",
    "from inception.slim import scopes\n",
    "from inception.slim import variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scopes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a466ba17b63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;33m@\u001b[0m\u001b[0mscopes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_arg_scope\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m def batch_norm(inputs,\n\u001b[1;32m      7\u001b[0m                \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scopes' is not defined"
     ]
    }
   ],
   "source": [
    "# Used to keep the update ops done by batch_norm.\n",
    "UPDATE_OPS_COLLECTION = '_update_ops_'\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def batch_norm(inputs,\n",
    "               decay=0.999,\n",
    "               center=True,\n",
    "               scale=False,\n",
    "               epsilon=0.001,\n",
    "               moving_vars='moving_vars',\n",
    "               activation=None,\n",
    "               is_training=True,\n",
    "               trainable=True,\n",
    "               restore=True,\n",
    "               scope=None,\n",
    "               reuse=None):\n",
    "  \"\"\"Adds a Batch Normalization layer.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels]\n",
    "            or [batch_size, channels].\n",
    "    decay: decay for the moving average.\n",
    "    center: If True, subtract beta. If False, beta is not created and ignored.\n",
    "    scale: If True, multiply by gamma. If False, gamma is\n",
    "      not used. When the next layer is linear (also e.g. ReLU), this can be\n",
    "      disabled since the scaling can be done by the next layer.\n",
    "    epsilon: small float added to variance to avoid dividing by zero.\n",
    "    moving_vars: collection to store the moving_mean and moving_variance.\n",
    "    activation: activation function.\n",
    "    is_training: whether or not the model is in training mode.\n",
    "    trainable: whether or not the variables should be trainable or not.\n",
    "    restore: whether or not the variables should be marked for restore.\n",
    "    scope: Optional scope for variable_scope.\n",
    "    reuse: whether or not the layer and its variables should be reused. To be\n",
    "      able to reuse the layer scope must be given.\n",
    "  Returns:\n",
    "    a tensor representing the output of the operation.\n",
    "  \"\"\"\n",
    "  inputs_shape = inputs.get_shape()\n",
    "  with tf.variable_scope(scope, 'BatchNorm', [inputs], reuse=reuse):\n",
    "    axis = list(range(len(inputs_shape) - 1))\n",
    "    params_shape = inputs_shape[-1:]\n",
    "    # Allocate parameters for the beta and gamma of the normalization.\n",
    "    beta, gamma = None, None\n",
    "    if center:\n",
    "      beta = variables.variable('beta',\n",
    "                                params_shape,\n",
    "                                initializer=tf.zeros_initializer(),\n",
    "                                trainable=trainable,\n",
    "                                restore=restore)\n",
    "    if scale:\n",
    "      gamma = variables.variable('gamma',\n",
    "                                 params_shape,\n",
    "                                 initializer=tf.ones_initializer(),\n",
    "                                 trainable=trainable,\n",
    "                                 restore=restore)\n",
    "    # Create moving_mean and moving_variance add them to\n",
    "    # GraphKeys.MOVING_AVERAGE_VARIABLES collections.\n",
    "    moving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES]\n",
    "    moving_mean = variables.variable('moving_mean',\n",
    "                                     params_shape,\n",
    "                                     initializer=tf.zeros_initializer(),\n",
    "                                     trainable=False,\n",
    "                                     restore=restore,\n",
    "                                     collections=moving_collections)\n",
    "    moving_variance = variables.variable('moving_variance',\n",
    "                                         params_shape,\n",
    "                                         initializer=tf.ones_initializer(),\n",
    "                                         trainable=False,\n",
    "                                         restore=restore,\n",
    "                                         collections=moving_collections)\n",
    "    if is_training:\n",
    "      # Calculate the moments based on the individual batch.\n",
    "      mean, variance = tf.nn.moments(inputs, axis)\n",
    "\n",
    "      update_moving_mean = moving_averages.assign_moving_average(\n",
    "          moving_mean, mean, decay)\n",
    "      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n",
    "      update_moving_variance = moving_averages.assign_moving_average(\n",
    "          moving_variance, variance, decay)\n",
    "      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n",
    "    else:\n",
    "      # Just use the moving_mean and moving_variance.\n",
    "      mean = moving_mean\n",
    "      variance = moving_variance\n",
    "    # Normalize the activations.\n",
    "    outputs = tf.nn.batch_normalization(\n",
    "        inputs, mean, variance, beta, gamma, epsilon)\n",
    "    outputs.set_shape(inputs.get_shape())\n",
    "    if activation:\n",
    "      outputs = activation(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _two_element_tuple(int_or_tuple):\n",
    "  \"\"\"Converts `int_or_tuple` to height, width.\n",
    "  Several of the functions that follow accept arguments as either\n",
    "  a tuple of 2 integers or a single integer.  A single integer\n",
    "  indicates that the 2 values of the tuple are the same.\n",
    "  This functions normalizes the input value by always returning a tuple.\n",
    "  Args:\n",
    "    int_or_tuple: A list of 2 ints, a single int or a tf.TensorShape.\n",
    "  Returns:\n",
    "    A tuple with 2 values.\n",
    "  Raises:\n",
    "    ValueError: If `int_or_tuple` it not well formed.\n",
    "  \"\"\"\n",
    "  if isinstance(int_or_tuple, (list, tuple)):\n",
    "    if len(int_or_tuple) != 2:\n",
    "      raise ValueError('Must be a list with 2 elements: %s' % int_or_tuple)\n",
    "    return int(int_or_tuple[0]), int(int_or_tuple[1])\n",
    "  if isinstance(int_or_tuple, int):\n",
    "    return int(int_or_tuple), int(int_or_tuple)\n",
    "  if isinstance(int_or_tuple, tf.TensorShape):\n",
    "    if len(int_or_tuple) == 2:\n",
    "      return int_or_tuple[0], int_or_tuple[1]\n",
    "  raise ValueError('Must be an int, a list with 2 elements or a TensorShape of '\n",
    "                   'length 2')\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def conv2d(inputs,\n",
    "           num_filters_out,\n",
    "           kernel_size,\n",
    "           stride=1,\n",
    "           padding='SAME',\n",
    "           activation=tf.nn.relu,\n",
    "           stddev=0.01,\n",
    "           bias=0.0,\n",
    "           weight_decay=0,\n",
    "           batch_norm_params=None,\n",
    "           is_training=True,\n",
    "           trainable=True,\n",
    "           restore=True,\n",
    "           scope=None,\n",
    "           reuse=None):\n",
    "  \"\"\"Adds a 2D convolution followed by an optional batch_norm layer.\n",
    "  conv2d creates a variable called 'weights', representing the convolutional\n",
    "  kernel, that is convolved with the input. If `batch_norm_params` is None, a\n",
    "  second variable called 'biases' is added to the result of the convolution\n",
    "  operation.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_filters_out: the number of output filters.\n",
    "    kernel_size: a list of length 2: [kernel_height, kernel_width] of\n",
    "      of the filters. Can be an int if both values are the same.\n",
    "    stride: a list of length 2: [stride_height, stride_width].\n",
    "      Can be an int if both strides are the same.  Note that presently\n",
    "      both strides must have the same value.\n",
    "    padding: one of 'VALID' or 'SAME'.\n",
    "    activation: activation function.\n",
    "    stddev: standard deviation of the truncated guassian weight distribution.\n",
    "    bias: the initial value of the biases.\n",
    "    weight_decay: the weight decay.\n",
    "    batch_norm_params: parameters for the batch_norm. If is None don't use it.\n",
    "    is_training: whether or not the model is in training mode.\n",
    "    trainable: whether or not the variables should be trainable or not.\n",
    "    restore: whether or not the variables should be marked for restore.\n",
    "    scope: Optional scope for variable_scope.\n",
    "    reuse: whether or not the layer and its variables should be reused. To be\n",
    "      able to reuse the layer scope must be given.\n",
    "  Returns:\n",
    "    a tensor representing the output of the operation.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'Conv', [inputs], reuse=reuse):\n",
    "    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n",
    "    stride_h, stride_w = _two_element_tuple(stride)\n",
    "    num_filters_in = inputs.get_shape()[-1]\n",
    "    weights_shape = [kernel_h, kernel_w,\n",
    "                     num_filters_in, num_filters_out]\n",
    "    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    l2_regularizer = None\n",
    "    if weight_decay and weight_decay > 0:\n",
    "      l2_regularizer = losses.l2_regularizer(weight_decay)\n",
    "    weights = variables.variable('weights',\n",
    "                                 shape=weights_shape,\n",
    "                                 initializer=weights_initializer,\n",
    "                                 regularizer=l2_regularizer,\n",
    "                                 trainable=trainable,\n",
    "                                 restore=restore)\n",
    "    conv = tf.nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1],\n",
    "                        padding=padding)\n",
    "    if batch_norm_params is not None:\n",
    "      with scopes.arg_scope([batch_norm], is_training=is_training,\n",
    "                            trainable=trainable, restore=restore):\n",
    "        outputs = batch_norm(conv, **batch_norm_params)\n",
    "    else:\n",
    "      bias_shape = [num_filters_out,]\n",
    "      bias_initializer = tf.constant_initializer(bias)\n",
    "      biases = variables.variable('biases',\n",
    "                                  shape=bias_shape,\n",
    "                                  initializer=bias_initializer,\n",
    "                                  trainable=trainable,\n",
    "                                  restore=restore)\n",
    "      outputs = tf.nn.bias_add(conv, biases)\n",
    "    if activation:\n",
    "      outputs = activation(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def fc(inputs,\n",
    "       num_units_out,\n",
    "       activation=tf.nn.relu,\n",
    "       stddev=0.01,\n",
    "       bias=0.0,\n",
    "       weight_decay=0,\n",
    "       batch_norm_params=None,\n",
    "       is_training=True,\n",
    "       trainable=True,\n",
    "       restore=True,\n",
    "       scope=None,\n",
    "       reuse=None):\n",
    "  \"\"\"Adds a fully connected layer followed by an optional batch_norm layer.\n",
    "  FC creates a variable called 'weights', representing the fully connected\n",
    "  weight matrix, that is multiplied by the input. If `batch_norm` is None, a\n",
    "  second variable called 'biases' is added to the result of the initial\n",
    "  vector-matrix multiplication.\n",
    "  Args:\n",
    "    inputs: a [B x N] tensor where B is the batch size and N is the number of\n",
    "            input units in the layer.\n",
    "    num_units_out: the number of output units in the layer.\n",
    "    activation: activation function.\n",
    "    stddev: the standard deviation for the weights.\n",
    "    bias: the initial value of the biases.\n",
    "    weight_decay: the weight decay.\n",
    "    batch_norm_params: parameters for the batch_norm. If is None don't use it.\n",
    "    is_training: whether or not the model is in training mode.\n",
    "    trainable: whether or not the variables should be trainable or not.\n",
    "    restore: whether or not the variables should be marked for restore.\n",
    "    scope: Optional scope for variable_scope.\n",
    "    reuse: whether or not the layer and its variables should be reused. To be\n",
    "      able to reuse the layer scope must be given.\n",
    "  Returns:\n",
    "     the tensor variable representing the result of the series of operations.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'FC', [inputs], reuse=reuse):\n",
    "    num_units_in = inputs.get_shape()[1]\n",
    "    weights_shape = [num_units_in, num_units_out]\n",
    "    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    l2_regularizer = None\n",
    "    if weight_decay and weight_decay > 0:\n",
    "      l2_regularizer = losses.l2_regularizer(weight_decay)\n",
    "    weights = variables.variable('weights',\n",
    "                                 shape=weights_shape,\n",
    "                                 initializer=weights_initializer,\n",
    "                                 regularizer=l2_regularizer,\n",
    "                                 trainable=trainable,\n",
    "                                 restore=restore)\n",
    "    if batch_norm_params is not None:\n",
    "      outputs = tf.matmul(inputs, weights)\n",
    "      with scopes.arg_scope([batch_norm], is_training=is_training,\n",
    "                            trainable=trainable, restore=restore):\n",
    "        outputs = batch_norm(outputs, **batch_norm_params)\n",
    "    else:\n",
    "      bias_shape = [num_units_out,]\n",
    "      bias_initializer = tf.constant_initializer(bias)\n",
    "      biases = variables.variable('biases',\n",
    "                                  shape=bias_shape,\n",
    "                                  initializer=bias_initializer,\n",
    "                                  trainable=trainable,\n",
    "                                  restore=restore)\n",
    "      outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n",
    "    if activation:\n",
    "      outputs = activation(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels, num_classes, scope=None):\n",
    "  \"\"\"Transform numeric labels into onehot_labels.\n",
    "  Args:\n",
    "    labels: [batch_size] target labels.\n",
    "    num_classes: total number of classes.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    one hot encoding of the labels.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'OneHotEncoding', [labels]):\n",
    "    batch_size = labels.get_shape()[0]\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "    labels = tf.cast(tf.expand_dims(labels, 1), indices.dtype)\n",
    "    concated = tf.concat(axis=1, values=[indices, labels])\n",
    "    onehot_labels = tf.sparse_to_dense(\n",
    "        concated, tf.stack([batch_size, num_classes]), 1.0, 0.0)\n",
    "    onehot_labels.set_shape([batch_size, num_classes])\n",
    "    return onehot_labels\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def max_pool(inputs, kernel_size, stride=2, padding='VALID', scope=None):\n",
    "  \"\"\"Adds a Max Pooling layer.\n",
    "  It is assumed by the wrapper that the pooling is only done per image and not\n",
    "  in depth or batch.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, depth].\n",
    "    kernel_size: a list of length 2: [kernel_height, kernel_width] of the\n",
    "      pooling kernel over which the op is computed. Can be an int if both\n",
    "      values are the same.\n",
    "    stride: a list of length 2: [stride_height, stride_width].\n",
    "      Can be an int if both strides are the same.  Note that presently\n",
    "      both strides must have the same value.\n",
    "    padding: the padding method, either 'VALID' or 'SAME'.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a tensor representing the results of the pooling operation.\n",
    "  Raises:\n",
    "    ValueError: if 'kernel_size' is not a 2-D list\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'MaxPool', [inputs]):\n",
    "    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n",
    "    stride_h, stride_w = _two_element_tuple(stride)\n",
    "    return tf.nn.max_pool(inputs,\n",
    "                          ksize=[1, kernel_h, kernel_w, 1],\n",
    "                          strides=[1, stride_h, stride_w, 1],\n",
    "                          padding=padding)\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def avg_pool(inputs, kernel_size, stride=2, padding='VALID', scope=None):\n",
    "  \"\"\"Adds a Avg Pooling layer.\n",
    "  It is assumed by the wrapper that the pooling is only done per image and not\n",
    "  in depth or batch.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, depth].\n",
    "    kernel_size: a list of length 2: [kernel_height, kernel_width] of the\n",
    "      pooling kernel over which the op is computed. Can be an int if both\n",
    "      values are the same.\n",
    "    stride: a list of length 2: [stride_height, stride_width].\n",
    "      Can be an int if both strides are the same.  Note that presently\n",
    "      both strides must have the same value.\n",
    "    padding: the padding method, either 'VALID' or 'SAME'.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a tensor representing the results of the pooling operation.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'AvgPool', [inputs]):\n",
    "    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n",
    "    stride_h, stride_w = _two_element_tuple(stride)\n",
    "    return tf.nn.avg_pool(inputs,\n",
    "                          ksize=[1, kernel_h, kernel_w, 1],\n",
    "                          strides=[1, stride_h, stride_w, 1],\n",
    "                          padding=padding)\n",
    "\n",
    "\n",
    "@scopes.add_arg_scope\n",
    "def dropout(inputs, keep_prob=0.5, is_training=True, scope=None):\n",
    "  \"\"\"Returns a dropout layer applied to the input.\n",
    "  Args:\n",
    "    inputs: the tensor to pass to the Dropout layer.\n",
    "    keep_prob: the probability of keeping each input unit.\n",
    "    is_training: whether or not the model is in training mode. If so, dropout is\n",
    "    applied and values scaled. Otherwise, inputs is returned.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a tensor representing the output of the operation.\n",
    "  \"\"\"\n",
    "  if is_training and keep_prob > 0:\n",
    "    with tf.name_scope(scope, 'Dropout', [inputs]):\n",
    "      return tf.nn.dropout(inputs, keep_prob)\n",
    "  else:\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def flatten(inputs, scope=None):\n",
    "  \"\"\"Flattens the input while maintaining the batch_size.\n",
    "    Assumes that the first dimension represents the batch.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, ...].\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    a flattened tensor with shape [batch_size, k].\n",
    "  Raises:\n",
    "    ValueError: if inputs.shape is wrong.\n",
    "  \"\"\"\n",
    "  if len(inputs.get_shape()) < 2:\n",
    "    raise ValueError('Inputs must be have a least 2 dimensions')\n",
    "  dims = inputs.get_shape()[1:]\n",
    "  k = dims.num_elements()\n",
    "  with tf.name_scope(scope, 'Flatten', [inputs]):\n",
    "    return tf.reshape(inputs, [-1, k])\n",
    "\n",
    "\n",
    "def repeat_op(repetitions, inputs, op, *args, **kwargs):\n",
    "  \"\"\"Build a sequential Tower starting from inputs by using an op repeatedly.\n",
    "  It creates new scopes for each operation by increasing the counter.\n",
    "  Example: given repeat_op(3, _, ops.conv2d, 64, [3, 3], scope='conv1')\n",
    "    it will repeat the given op under the following variable_scopes:\n",
    "      conv1/Conv\n",
    "      conv1/Conv_1\n",
    "      conv1/Conv_2\n",
    "  Args:\n",
    "    repetitions: number or repetitions.\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    op: an operation.\n",
    "    *args: args for the op.\n",
    "    **kwargs: kwargs for the op.\n",
    "  Returns:\n",
    "    a tensor result of applying the operation op, num times.\n",
    "  Raises:\n",
    "    ValueError: if the op is unknown or wrong.\n",
    "  \"\"\"\n",
    "  scope = kwargs.pop('scope', None)\n",
    "  with tf.variable_scope(scope, 'RepeatOp', [inputs]):\n",
    "    tower = inputs\n",
    "    for _ in range(repetitions):\n",
    "      tower = op(tower, *args, **kwargs)\n",
    "    return tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
